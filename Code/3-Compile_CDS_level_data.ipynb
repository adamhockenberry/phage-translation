{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "\n",
    "The purpose of this notebook is to create a set of `.tsv` files from various sources for both host and viral genomes. It is expected that the project organization follows the relative path where all initial files are located within `../Data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Using one custom library here that can be cloned at: https://github.com/amarallab/iCUB \n",
    "\n",
    "`iCUB` is essentially an improved effective number of codons where lower values indicate more biased sequences. See: https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0667 for details.\n",
    "\n",
    "Additionally, running this code also assumes that you have `ViennaRNA` installed system-wide as it will call `RNAfold` via the `subprocess` library to determine mRNA secondary structure values for each coding sequence. This code also relies on a previously created file mapping sequence hexamers to aSD (`5'-CCUCCU-3'`) binding strengths that is located in `../Data/energy_files/energyRef_CCUCCU_ensemble_noneConstraint.json`. This file was created with `RNAcofold` and contains the binding strengths for all 4,096 hexamers to the `5'-CCUCCU-3'` fragment.\n",
    "\n",
    "I additionally manually downloaded the relvant host genome `genbank` files and placed these in `../Data/NCBI_phage_db/host_genomes/`. The code assumes that you have done the same. All other relevant files should have been previously created in prior notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.append('../../iCUB/')\n",
    "import iCUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First setting up the very basic sequence `.tsv` files for each phage genome where each row is a single CDS\n",
    "\n",
    "Am also adding `CDS_density` and `CDS_number` columns to the full dataframe in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use this to select the representatives from each taxa that I care about**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../Data/NCBI_phage_db/paper_dataset_11_2020_with_clusters.tsv', sep='\\t')\n",
    "full_df['cluster_representative'] = 0\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_species_list = [562] #For building the pipeline\n",
    "host_species_list = list(set(full_df['Host_species_id']))\n",
    "print(host_species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df[full_df['Host_species_id']==562].shape)\n",
    "print(full_df[(full_df['Host_species_id']==562) & (full_df['ranking_in_cluster']==1)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This will take quite a while to run, it's not the most efficient thing in the world but does the trick**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxon_id in host_species_list:\n",
    "    print('TAXA:', taxon_id)\n",
    "    #Select the dataframe relevant to the particular species under investigation\n",
    "    species_df = full_df[(full_df['Host_species_id']==taxon_id)]\n",
    "    print('#####', species_df.shape)\n",
    "    cluster_ids = sorted(list(set(list(species_df['arbitrary_cluster_id']))))\n",
    "    for new_col in ['cluster_representative', 'CDS_density', 'CDS_number']:\n",
    "        full_df.at[species_df.index, new_col] = np.nan\n",
    "        \n",
    "    for cluster_id in cluster_ids:\n",
    "        print(cluster_id)\n",
    "        cluster_df = species_df[species_df['arbitrary_cluster_id']==cluster_id]\n",
    "        cluster_df = cluster_df.sort_values('ranking_in_cluster', ascending=True)\n",
    "        for index in cluster_df.index:\n",
    "            genome_id = full_df.loc[index]['Accession']\n",
    "            print(genome_id)\n",
    "            genome_nt = list(SeqIO.parse('../Data/NCBI_phage_db/phage_genomes/'\n",
    "                                         '{}_phage_genomes/{}.fasta'.format(taxon_id, genome_id), 'fasta'))\n",
    "            assert len(genome_nt) == 1\n",
    "            genome_nt = genome_nt[0]\n",
    "            nt_seq_len = len(str(genome_nt.seq))\n",
    "            assert nt_seq_len == full_df.loc[index]['Length']\n",
    "\n",
    "            #Painstakingly iterate through the very large set of sequence records to find hits for\n",
    "            #this particular record. I could keep this all in memory as a dictionary so that I only\n",
    "            #have to iterate through it once but it would have quite a large RAM requirement\n",
    "            hits = []\n",
    "            all_cds = SeqIO.parse('../Data/NCBI_phage_db/'\n",
    "                                      'all_complete_phage_CDSs_11_2020.fasta', 'fasta')\n",
    "            for cds in all_cds:\n",
    "                if genome_id in cds.id.split(':')[0]:\n",
    "                    hits.append(cds)\n",
    "            #Now iterate through those hits to extract out the sequence and its upstream region\n",
    "            listy = []\n",
    "            for hit in hits:\n",
    "                reverse_complement = False\n",
    "                if 'join' in hit.id: #Just skipping complex CDSs entirely\n",
    "                    continue\n",
    "                if 'complement(' == hit.id[:11]:\n",
    "                    reverse_complement = True\n",
    "                if len(hit.id.split(':')) == 2:\n",
    "                    region = hit.id.split(':')[-1]\n",
    "                    if region.count('<') + region.count('>') > 0:\n",
    "                        print('Found an annoying region definition')\n",
    "                        continue\n",
    "                    if len(region.split('..')) == 2:\n",
    "                        start = int(region.split('..')[0])\n",
    "                        if reverse_complement:\n",
    "                            stop = int(region.split('..')[-1][:-1])\n",
    "                        else:\n",
    "                            stop = int(region.split('..')[-1])\n",
    "                        assert stop > start\n",
    "                    else:\n",
    "                        print('Should not be here, possible bug to investigate')\n",
    "                else:\n",
    "                    print('Should not be here, possible bug to investigate')\n",
    "\n",
    "                #Ensuring that what I extracted matches the CDS record before advancing\n",
    "                assert str(genome_nt.seq[start-1:stop])==str(hit.seq)\n",
    "\n",
    "                if reverse_complement:\n",
    "                    if stop + 30 > nt_seq_len:\n",
    "                        continue\n",
    "                    strand = '-'\n",
    "                    cds_seq = str(genome_nt.seq[start-1:stop].reverse_complement())\n",
    "                    us_seq = str(genome_nt.seq[stop:stop+30].reverse_complement())\n",
    "                else:\n",
    "                    if start - 31 < 0:\n",
    "                        continue\n",
    "                    strand = '+'\n",
    "                    cds_seq = str(genome_nt.seq[start-1:stop])\n",
    "                    us_seq = str(genome_nt.seq[start-31:start-1])\n",
    "\n",
    "                ###Add the relevant information to a growing list    \n",
    "                listy.append([genome_id,\n",
    "                              start, \n",
    "                              stop,\n",
    "                              strand,\n",
    "                              cds_seq,\n",
    "                              us_seq])\n",
    "\n",
    "            #Get basic summary stats about the record set that I found\n",
    "            cds_len = 0\n",
    "            for i in listy:\n",
    "                cds_len += (i[2]-i[1])\n",
    "            temp_density = cds_len / full_df.loc[index]['Length']\n",
    "            \n",
    "            if temp_density > 0.5:\n",
    "                full_df.at[index, 'cluster_representative'] = 1\n",
    "                full_df.at[index, 'CDS_density'] = temp_density\n",
    "                full_df.at[index, 'CDS_number'] = len(listy)\n",
    "\n",
    "                #Save the individual set\n",
    "                single_df = pd.DataFrame(listy,\n",
    "                                         columns = ['Genome_source', 'Start', 'Stop', 'Strand', 'CDS_seq', 'Upstream_seq'])\n",
    "                single_df.to_csv('../Data/NCBI_phage_db/phage_genomes/'\n",
    "                                 '{}_phage_genomes/{}.tsv'.format(taxon_id, genome_id),\n",
    "                                sep='\\t', index=False)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('../Data/NCBI_phage_db/'\n",
    "               'paper_dataset_11_2020_with_clusters.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore a bit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['Host_species_id']==taxon_id)\n",
    "        & (full_df['cluster_representative']==1)].sort_values('CDS_number').head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_species in host_species_list:\n",
    "    print('###', host_species)\n",
    "    species_df = full_df[full_df['Host_species_id']==host_species]\n",
    "    print(species_df.shape[0],\n",
    "          max(species_df['arbitrary_cluster_id']),\n",
    "          species_df[species_df['cluster_representative']==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And repeat the above code to compile `.tsv` files for each host genome as well\n",
    "\n",
    "As noted in the code, I'm removing any CDS whose description contains the word `phage`. Hopefully this will remove some prophage genes and make for a cleaner analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_word = 'phage' #For ease, remove anything of, or pertaining to, the string 'phage'\n",
    "for host_species in host_species_list:\n",
    "    genome_locs = glob.glob('../Data/NCBI_phage_db/host_genomes/{}*.gb'.format(host_species))\n",
    "    print(genome_locs)\n",
    "#     genome_id = genome_locs[0].split('/')[-1].split('.')[0]\n",
    "#     print(genome_id)\n",
    "    listy = []\n",
    "    for genome_loc in genome_locs:\n",
    "        print(genome_loc)\n",
    "        genome_nt = list(SeqIO.parse(genome_loc, 'genbank'))\n",
    "        assert len(genome_nt) == 1\n",
    "        genome_nt = genome_nt[0]\n",
    "        genome_id = genome_nt.id\n",
    "        print(genome_id)\n",
    "        nt_seq_len = len(str(genome_nt.seq))\n",
    "        for feature in genome_nt.features:\n",
    "            if feature.type == 'CDS':\n",
    "                if filter_word in feature.qualifiers['product'][0].lower():\n",
    "                    continue\n",
    "                try:\n",
    "                    locus_id = feature.qualifiers['locus_tag'][0]\n",
    "                except:\n",
    "                    locus_id = ''\n",
    "                try:\n",
    "                    gene_id = feature.qualifiers['gene'][0]\n",
    "                except:\n",
    "                    gene_id = ''\n",
    "                start = feature.location.start\n",
    "                stop = feature.location.end\n",
    "                strand = feature.location.strand\n",
    "                if strand == 1:\n",
    "                    strand = '+'\n",
    "                elif strand == -1:\n",
    "                    strand = '-'\n",
    "                else:\n",
    "                    print('ERROR')\n",
    "                    break\n",
    "                if strand == '-':\n",
    "                    if stop + 30 > nt_seq_len:\n",
    "                        continue\n",
    "                    listy.append([genome_id,\n",
    "                                  start, \n",
    "                                  stop, \n",
    "                                  strand,\n",
    "                                  str(genome_nt.seq[start:stop].reverse_complement()),\n",
    "                                  str(genome_nt.seq[stop:stop+30].reverse_complement()),\n",
    "                                  locus_id,\n",
    "                                  gene_id])\n",
    "                elif strand == '+':\n",
    "                    if start < 30:\n",
    "                        continue\n",
    "                    listy.append([genome_id,\n",
    "                                  start, \n",
    "                                  stop,\n",
    "                                  strand,\n",
    "                                  str(genome_nt.seq[start:stop]),\n",
    "                                  str(genome_nt.seq[start-30:start]),\n",
    "                                  locus_id,\n",
    "                                  gene_id])\n",
    "                else:\n",
    "                    print('MAJOR ERROR')\n",
    "                    break\n",
    "    if len(genome_locs) != 0:\n",
    "        print(len(listy))\n",
    "        single_df = pd.DataFrame(listy,\n",
    "                                 columns = ['Genome_source', 'Start', 'Stop', 'Strand', 'CDS_seq', 'Upstream_seq', 'locus_tag', 'gene_id'])\n",
    "        single_df.to_csv('../Data/NCBI_phage_db/'\n",
    "                         'host_genomes/{}.tsv'.format(host_species),\n",
    "                        sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom function definitions to add new columns to host and phage `.tsv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_RBS_energy(df, energy_dict, col_name='aSD_binding',\\\n",
    "                   gaps=(4,10), expected_len=30, RBS_len=6):\n",
    "    '''\n",
    "    This function adds a ribosome binding site (RBS) energy column to the df based off of \n",
    "    pre-computed free energy vals (created with RNAcofold and stored in the corresponding \n",
    "    energy_dict. \n",
    "    \n",
    "    Inputs:\n",
    "        df - the single df for a given organism where each row is a CDS\n",
    "        energy_dict - dictionary of aSD binding energies for all k-mers (where k is defined in RBS_len)\n",
    "        gaps - the allowable gap between 3' end of the sequence and the start codon\n",
    "        expected_len - a check on how long I expect the k-mers to be\n",
    "    \n",
    "    Outputs:\n",
    "        df - the transformed df object now containing the new column \"col_name\"\n",
    "    '''\n",
    "    for index in df.index:\n",
    "        upstream = df.loc[index,'Upstream_seq']\n",
    "        test_string = upstream.replace('T', 'U')\n",
    "        ###Ensure that the sequence is the proper expected length\n",
    "        if len(test_string) != expected_len:\n",
    "            continue\n",
    "        ###Ensure that the sequence has no abnormal bases\n",
    "        if test_string.count('A') + test_string.count('U') +\\\n",
    "                                    test_string.count('C') + test_string.count('G') != expected_len:\n",
    "            continue\n",
    "            \n",
    "        ###Calculate the energy for the indicated gap offsets\n",
    "        energy_list = []\n",
    "        for gap in range(gaps[0],gaps[1]+1):\n",
    "             energy_list.append(energy_dict[test_string[-gap - RBS_len: -gap]])\n",
    "\n",
    "        df.at[index, col_name] = min(energy_list)        \n",
    "    return df\n",
    "\n",
    "\n",
    "def call_RNAfold(sequence):\n",
    "    '''\n",
    "    This function is really just to call RNAfold on the given string \n",
    "    \n",
    "    Inputs:\n",
    "        sequence - a gene sequence (string)\n",
    "    \n",
    "    Outputs:\n",
    "        stdout - subprocess output from the RNAfold call\n",
    "    '''\n",
    "    \n",
    "    sequence = sequence.replace('T', 'U')\n",
    "    MyOut = subprocess.Popen(['RNAfold', '-p', '--noPS', '--noDP', '--constraint'],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.STDOUT)\n",
    "    stdout, stderr = MyOut.communicate(input=str.encode(sequence))\n",
    "    return stdout\n",
    "\n",
    "def get_energy_RNAfold(stdout_string):\n",
    "    '''\n",
    "    Parses the RNAfold stdout \n",
    "    \n",
    "    Inputs:\n",
    "        stdout_string - coming directly from call_RNAfold\n",
    "    \n",
    "    Outputs:\n",
    "        ensemble_val - float representing the ensemble free energy\n",
    "        mfe_val - float representing the minimum free energy\n",
    "\n",
    "    '''\n",
    "    temp = stdout_string.decode('utf-8') \n",
    "    mfe_line = temp.split('\\n')[-5]\n",
    "    mfe_val = mfe_line[mfe_line.index(' '):]\n",
    "    mfe_val = mfe_val.strip().strip('()').strip()\n",
    "    #\n",
    "    ensemble_line = temp.split('\\n')[-4]\n",
    "    ensemble_val = ensemble_line[ensemble_line.index(' '):]\n",
    "    ensemble_val = ensemble_val.strip().strip('[]').strip()\n",
    "    return float(ensemble_val), float(mfe_val)    \n",
    "\n",
    "def add_secondary_structure(df):\n",
    "    '''\n",
    "    This function adds an mRNA secondary structure strength column to the df using subprocess\n",
    "    calls to RNAfold. Requires that df['Upstream_seq'] and df['CDS_seq'] be defined.\n",
    "    \n",
    "    Inputs:\n",
    "        df - the single df for a given organism where each row is a CDS\n",
    "        \n",
    "    Outputs:\n",
    "        df - the transformed df object now containing the new columns \"sec_struct\" and \"sec_struct_bound\"\n",
    "    '''\n",
    "    for index in df.index:\n",
    "        us_seq = df.loc[index]['Upstream_seq'] \n",
    "        cds_seq = df.loc[index]['CDS_seq']\n",
    "        if len(cds_seq) < 90:\n",
    "            continue\n",
    "        beg_cds_seq = cds_seq[:60]\n",
    "        if len(us_seq) == 30 and len(beg_cds_seq) == 60:\n",
    "            seq = us_seq + beg_cds_seq\n",
    "            rna_out = call_RNAfold(seq)\n",
    "            e1, e2 = get_energy_RNAfold(rna_out)\n",
    "            df.at[index, 'sec_struct'] = e1\n",
    "            #\n",
    "            rna_out = call_RNAfold(seq+'\\n'+ribo_bound_constraint)\n",
    "            e1, e2 = get_energy_RNAfold(rna_out)\n",
    "            df.at[index, 'sec_struct_bound'] = e1\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_iCUB_and_GC(df):\n",
    "    '''\n",
    "    This function adds an iCUB column to the df as well as a simple GC content column.\n",
    "    Requires that df['Upstream_seq'] and df['CDS_seq'] be defined.\n",
    "    \n",
    "    Inputs:\n",
    "        df - the single df for a given organism where each row is a CDS\n",
    "        \n",
    "    Outputs:\n",
    "        df - the transformed df object now containing the new columns \"iCUB\", \"GC_cds\", and \"GC_upstream\"\n",
    "    '''\n",
    "    for index in df.index:\n",
    "        cds_seq = df.loc[index]['CDS_seq']\n",
    "        if len(cds_seq) == 0:\n",
    "            continue\n",
    "        if len(cds_seq) != cds_seq.count('A') + cds_seq.count('T') + cds_seq.count('C') + cds_seq.count('G'):\n",
    "            continue\n",
    "        if len(cds_seq)%3 != 0:\n",
    "            continue\n",
    "        df.at[index, 'iCUB'] = iCUB.iCUB_Calculator(cds_seq).get_iCUB()\n",
    "        #\n",
    "        df.at[index, 'GC_cds'] = (cds_seq.count('G') + cds_seq.count('C')) / len(cds_seq)\n",
    "        #\n",
    "        us_seq = df.loc[index]['Upstream_seq']\n",
    "        if len(us_seq) == 30:\n",
    "            df.at[index, 'GC_upstream'] = (us_seq.count('G') + us_seq.count('C')) / len(us_seq)\n",
    "    return df\n",
    "\n",
    "\n",
    "def common_cleaning(df):\n",
    "    '''\n",
    "    This function cleans the dataframe according to a few different columns that I wish to be non-null.\n",
    "    \n",
    "    Inputs:\n",
    "        df - the single df for a given organism where each row is a CDS\n",
    "        \n",
    "    Outputs:\n",
    "        df - the clean dataframe containing (ideally) no NaN entries in interesting columns.\n",
    "    '''\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in ['Upstream_seq', 'CDS_seq', 'iCUB', 'GC_cds', 'GC_upstream', 'aSD_binding', 'sec_struct']:\n",
    "        df = df[df[i].isnull()==False]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = '\\t'\n",
    "upstream_len = 30\n",
    "\n",
    "with open('../Data/energy_files/energyRef_CCUCCU_ensemble_noneConstraint.json', 'r') as infile:\n",
    "       energy_dict = json.load(infile)\n",
    "\n",
    "###For evaluating secondary structure given the constraint that the start codon region is forced\n",
    "###to be single stranded\n",
    "ribo_bound_constraint = ('.'*16) + ('x'*28)+ ('.'*46)\n",
    "assert len(ribo_bound_constraint)==90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending and cleaning `.tsv` files for host genomes\n",
    "\n",
    "**Adding relevant columns for ribosome binding sites, secondary structure, codon usage bias, and GC content. This whole process takes 5-10 minutes per genome. Could definitely be optimized but for now not a bottleneck, just anticipate a few hours for this cell to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_genome_dir = '../Data/NCBI_phage_db/host_genomes/'\n",
    "for host_id in host_species_list:\n",
    "    print(host_id)\n",
    "    if host_id == 562:\n",
    "        continue\n",
    "    try:\n",
    "        host_df = pd.read_csv(host_genome_dir + '{}.tsv'.format(host_id), sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        print('No genome with id:', host_id)\n",
    "        continue\n",
    "    ###Adds the ribosome binding site energy column\n",
    "    host_df = add_RBS_energy(host_df, energy_dict, col_name='aSD_binding', gaps=(4,10))\n",
    "\n",
    "    ###Adds the secondary structure column\n",
    "    host_df = add_secondary_structure(host_df)\n",
    "    \n",
    "    ###Add codon usage bias column\n",
    "    host_df = add_iCUB_and_GC(host_df)\n",
    "    \n",
    "    ###Clean things up\n",
    "    host_df = common_cleaning(host_df)\n",
    "    \n",
    "    host_df['Start_accessibility'] = host_df['sec_struct'] - host_df['sec_struct_bound']\n",
    "    \n",
    "    ###Writes to a file\n",
    "    host_df.to_csv(host_genome_dir + '{}.clean.tsv'.format(host_id), sep=sep, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore a bit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending and cleaning `.tsv` files for viral genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_viral_genome_dir = '../Data/NCBI_phage_db/phage_genomes/'\n",
    "\n",
    "for host_id in host_species_list:\n",
    "    print('#####', host_id)\n",
    "    if host_id == 562:\n",
    "        continue\n",
    "    species_df = full_df[(full_df['Host_species_id']==host_id) & (full_df['cluster_representative']==1)]\n",
    "    for index in species_df.index:\n",
    "        virus_id = species_df.loc[index]['Accession']\n",
    "        virus_file = base_viral_genome_dir+'{}_phage_genomes/{}.tsv'.format(host_id, virus_id)\n",
    "        print(virus_file)\n",
    "        viral_df = pd.read_csv(virus_file, sep='\\t')\n",
    "        print(viral_df.shape)\n",
    "        ###Adds the ribosome binding site energy column\n",
    "        viral_df = add_RBS_energy(viral_df, energy_dict, col_name='aSD_binding', gaps=(4,10))\n",
    "\n",
    "        ###Adds the secondary structure column\n",
    "        viral_df = add_secondary_structure(viral_df)\n",
    "\n",
    "        ###Add codon usage bias column\n",
    "        viral_df = add_iCUB_and_GC(viral_df)\n",
    "    \n",
    "        ###Clean things up\n",
    "        viral_df = common_cleaning(viral_df)\n",
    "            \n",
    "        viral_df['Start_accessibility'] = viral_df['sec_struct'] - viral_df['sec_struct_bound']\n",
    "        \n",
    "        ###Writes to a file\n",
    "        viral_df.to_csv(virus_file.replace('.tsv', '.clean.tsv'), sep=sep, index=False)\n",
    "        print(viral_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side analysis just to make sure that all these genomes use the standard translation table (because what a pain if any do not)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_gb in glob.glob('../Data/NCBI_phage_db/host_genomes/*.gb'):\n",
    "    print(host_gb)\n",
    "    translation_tables = []\n",
    "    genome_nt = list(SeqIO.parse(host_gb, 'genbank'))[0]\n",
    "    for feature in genome_nt.features:\n",
    "        if feature.type == 'CDS':\n",
    "            translation_tables.append(feature.qualifiers['transl_table'][0])\n",
    "    print(set(translation_tables), len(translation_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../Data/NCBI_phage_db/paper_dataset_11_2020_with_clusters.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['Sequence_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df[full_df['Host_species_id']==562].shape)\n",
    "print(full_df[(full_df['Host_species_id']==562) & (full_df['ranking_in_cluster']==1)].shape)\n",
    "print(full_df[(full_df['Host_species_id']==562) & (full_df['cluster_representative']==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
